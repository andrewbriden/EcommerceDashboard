# -*- coding: utf-8 -*-
"""ecom.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Khy3AtTd6HBli1JzWmZzLATlQSQo2Q_i
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Path to the directory containing your file
directory_path = '/content/drive/MyDrive/ecom'

# List files in the directory to confirm the path
import os
files = os.listdir(directory_path)
print("Files in directory:", files)

# Path to the dataset in your Google Drive
file_path = '/content/drive/MyDrive/ecom/online_retail_listing.csv'

# Load the dataset with the correct delimiter and encoding
df = pd.read_csv(file_path, delimiter=';', encoding='ISO-8859-1')

# Replace commas with dots in the 'Price' column and convert it to float
df['Price'] = df['Price'].str.replace(',', '.').astype(float)

# Parse the 'InvoiceDate' column to datetime
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], format='%d.%m.%Y %H:%M')

# Display the first few rows of the cleaned dataset
df.head()
df.info()



import pandas as pd

# Load the dataset with the correct delimiter and encoding
file_path = '/content/drive/MyDrive/ecom/online_retail_listing.csv'
df = pd.read_csv(file_path, delimiter=';', encoding='ISO-8859-1')

# Replace commas with dots in the 'Price' column and convert it to float
df['Price'] = df['Price'].str.replace(',', '.').astype(float)

# Parse the 'InvoiceDate' column to datetime
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], format='%d.%m.%Y %H:%M')

# Data cleaning steps
df.dropna(inplace=True)  # Drop rows with missing values
df.drop_duplicates(inplace=True)  # Remove duplicate rows

# Feature engineering
df['TotalPrice'] = df['Quantity'] * df['Price']

# Display the first few rows of the cleaned dataset
df.head()
df.info()

import seaborn as sns

# Extract hour from InvoiceDate
df['Hour'] = pd.to_datetime(df['InvoiceDate']).dt.hour

# Group by hour to find the count of orders per hour
orders_per_hour = df.groupby('Hour').size()

# Visualization using a bar plot
plt.figure(figsize=(10, 6))
sns.barplot(x=orders_per_hour.index, y=orders_per_hour.values, palette='viridis')
plt.title('Number of Orders Placed by Hour')
plt.xlabel('Hour of Day')
plt.ylabel('Number of Orders')
plt.show()

# Aggregate quantity by StockCode and Description
top_products = df.groupby(['StockCode', 'Description']).agg({'Quantity': 'sum'}).reset_index()
top_products = top_products.sort_values(by='Quantity', ascending=False).head(10)

# Visualization using a bar plot
plt.figure(figsize=(14, 7))
sns.barplot(x='Quantity', y='Description', data=top_products, palette='viridis')
plt.title('Top 10 Products by Quantity Sold')
plt.xlabel('Quantity Sold')
plt.ylabel('Product Description')
plt.show()

# Calculate total revenue per product
df['TotalRevenue'] = df['Quantity'] * df['Price']

# Aggregate total revenue by product
total_revenue = df.groupby(['StockCode', 'Description']).agg({'TotalRevenue': 'sum'}).reset_index()
total_revenue = total_revenue.sort_values(by='TotalRevenue', ascending=False).head(10)

# Display the top products by total revenue
print(total_revenue)

# Visualization using a bar plot
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(14, 7))
sns.barplot(x='TotalRevenue', y='Description', data=total_revenue, palette='viridis')
plt.title('Top 10 Products by Total Revenue')
plt.xlabel('Total Revenue (£)')
plt.ylabel('Product Description')
plt.show()

# Aggregate revenue by CustomerID
top_customers = df.groupby('Customer ID').agg({'TotalRevenue': 'sum'}).reset_index()
top_customers = top_customers.sort_values(by='TotalRevenue', ascending=False).head(10)

# Visualization using a bar plot
plt.figure(figsize=(14, 7))
sns.barplot(x='TotalRevenue', y='Customer ID', data=top_customers, palette='viridis')
plt.title('Top 10 Customers by Total Revenue')
plt.xlabel('Total Revenue (£)')
plt.ylabel('Customer ID')
plt.show()

# Aggregate revenue by Country
revenue_by_country = df.groupby('Country').agg({'TotalRevenue': 'sum'}).reset_index()
revenue_by_country = revenue_by_country.sort_values(by='TotalRevenue', ascending=False).head(10)

# Visualization using a bar plot
plt.figure(figsize=(14, 7))
sns.barplot(x='TotalRevenue', y='Country', data=revenue_by_country, palette='viridis')
plt.title('Top 10 Countries by Total Revenue')
plt.xlabel('Total Revenue (£)')
plt.ylabel('Country')
plt.show()

# Calculate average order value
average_order_value = df.groupby(pd.to_datetime(df['InvoiceDate']).dt.to_period('M')).agg({'TotalRevenue': 'mean'}).reset_index()
average_order_value['InvoiceDate'] = average_order_value['InvoiceDate'].astype(str)

# Visualization using a line plot
plt.figure(figsize=(14, 7))
sns.lineplot(x='InvoiceDate', y='TotalRevenue', data=average_order_value, marker='o')
plt.title('Average Order Value Over Time')
plt.xlabel('Month')
plt.ylabel('Average Order Value (£)')
plt.xticks(rotation=45)
plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt


# Convert InvoiceDate to datetime if it's not already
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])

# Add a column for the day of the week
df['DayOfWeek'] = df['InvoiceDate'].dt.day_name()

# Add a column for the hour of the day
df['Hour'] = df['InvoiceDate'].dt.hour

# Group by Hour and DayOfWeek to count the number of orders
orders_by_hour_day = df.groupby(['Hour', 'DayOfWeek']).size().reset_index(name='OrderCount')

# Pivot the data to create a matrix with Hours as rows and Days as columns
pivot_table = orders_by_hour_day.pivot(index='Hour', columns='DayOfWeek', values='OrderCount')

# Reorder the days of the week to start from Monday
days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
pivot_table = pivot_table[days_order]

# Fill NaN values with 0 for better visualization
pivot_table = pivot_table.fillna(0)

# Create the heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(pivot_table, cmap='Reds', annot=True, fmt=".0f", cbar_kws={'label': 'Number of Orders'})

# Customize the plot
plt.title('Heatmap of Orders by Hour and Day of the Week')
plt.xlabel('Day of the Week')
plt.ylabel('Hour of the Day')
plt.show()

# Install NLTK if it's not already installed
!pip install nltk

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re

# Download NLTK data
nltk.download('punkt')
nltk.download('stopwords')

import pandas as pd

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Path to the directory containing your file
directory_path = '/content/drive/MyDrive/ecom'

# List files in the directory to confirm the path
import os
files = os.listdir(directory_path)
print("Files in directory:", files)

# Path to the dataset in your Google Drive
file_path = '/content/drive/MyDrive/ecom/online_retail_listing.csv'

# Load the dataset with the correct delimiter and encoding
df = pd.read_csv(file_path, delimiter=';', encoding='ISO-8859-1')

# Convert the Description column to strings and handle missing values
df['Description'] = df['Description'].astype(str).fillna('')

# Text preprocessing function
def preprocess_text(text):
    # Lowercase the text
    text = text.lower()
    # Remove punctuation and numbers
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\d+', '', text)
    # Tokenize the text
    tokens = word_tokenize(text)
    # Remove stop words
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]
    # Join the tokens back into a single string
    text = ' '.join(tokens)
    return text

# Apply preprocessing to the description column
df['Description_clean'] = df['Description'].apply(preprocess_text)

# Vectorize the cleaned text data
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(df['Description_clean'])

# Perform clustering
kmeans = KMeans(n_clusters=10, random_state=42)  # Adjust the number of clusters as needed
df['Category'] = kmeans.fit_predict(X)

# Save categorized data to a new CSV file
df.to_csv('categorized_dataset.csv', index=False)

# Display the first few rows of the categorized data
df.head(10)

df.head(10)

# Analyze clusters to determine meaningful category names
for i in range(10):  # Adjust the range if the number of clusters is different
    print(f"Cluster {i}")
    print(df[df['Category'] == i]['Description'].head(10))
    print("\n")

# Create a mapping of cluster numbers to category names based on analysis
cluster_to_category = {
    0: "Category_A",
    1: "Category_B",
    2: "Category_C",
    3: "Category_D",
    4: "Category_E",
    5: "Category_F",
    6: "Category_G",
    7: "Category_H",
    8: "Category_I",
    9: "Category_J",
}

# Apply the mapping to create a new column with category names
df['Category_Name'] = df['Category'].map(cluster_to_category)

# Save categorized data to a new CSV file
df.to_csv('categorized_dataset_with_names.csv', index=False)

# Display the first few rows of the categorized data
df.head(10)

# Create a mapping of cluster numbers to category names based on analysis
cluster_to_category = {
    0: "Trinket Boxes and Decorative Boxes",
    1: "Bags and Storage",
    2: "Red Spot Items",
    3: "Paper Products",
    4: "Hanging T-Light Holders",
    5: "Design Items",
    6: "Cake Accessories",
    7: "Sets and Stickers",
    8: "Heart and Card Items",
    9: "Miscellaneous Decor",
}

# Apply the mapping to create a new column with category names
df['Category_Name'] = df['Category'].map(cluster_to_category)

# Save categorized data to a new CSV file
df.to_csv('categorized_dataset_with_names.csv', index=False)

# Display the first few rows of the categorized data
df.head(10)

df.to_csv('/content/drive/MyDrive/categorized_dataset_with_names.csv', index=False)